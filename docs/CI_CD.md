# Documentation CI/CD - Pipeline Big Data

## Vue d'Ensemble

Le workflow CI/CD GitHub Actions valide automatiquement la qualit√© du code, la configuration et la documentation √† chaque push ou pull request.

## Architecture du Workflow

```mermaid
graph TB
    subgraph "Validation Jobs (Parallel)"
        A[validate-config<br/>Docker Compose & .env]
        B[validate-code<br/>Python Syntax]
        C[validate-docs<br/>Markdown Files]
        D[validate-sql<br/>SQL Scripts]
    end
    
    subgraph "Integration Tests (Sequential)"
        E[integration-test<br/>Docker Build & Run]
    end
    
    subgraph "Security (Parallel)"
        F[security-scan<br/>Trivy Vulnerability]
    end
    
    A --> E
    B --> E
    C -.-> F
    D -.-> F
    
    style A fill:#4caf50
    style B fill:#2196f3
    style C fill:#ff9800
    style D fill:#9c27b0
    style E fill:#f44336
    style F fill:#607d8b
```

## Jobs D√©taill√©s

### 1. **validate-config** -
Valide la configuration Docker et les variables d'environnement.

**√âtapes :**
- V√©rification de l'existence de `.env.example`
- Validation de la syntaxe `docker-compose.yml` avec `docker compose config`

**Commande locale :**
```bash
docker compose config
```

### 2. **validate-code** üêç
Valide la syntaxe Python de tous les scripts.

**√âtapes :**
- Installation de Python 3.11
- Installation des d√©pendances (Airflow, PySpark, pymongo, kafka-python)
- Compilation de tous les fichiers `.py` :
  - Airflow DAGs (`airflow/dags/*.py`)
  - Scripts Spark (`spark/*.py`)
  - Scripts utilitaires (`src/*.py`)
  - Scripts producteurs (`producer/*.py`)

**Commande locale :**
```bash
pip install apache-airflow==2.7.2 pyspark==3.5.0 pymongo kafka-python
python -m py_compile airflow/dags/*.py src/*.py
```

### 3. **validate-docs** üìù
Valide l'existence et la syntaxe de la documentation.

**√âtapes :**
- V√©rification de `README.md` (obligatoire)
- V√©rification de `RAPPORT.md` (recommand√©)
- Linting Markdown avec `markdownlint`

**Commande locale :**
```bash
# Installer markdownlint-cli
npm install -g markdownlint-cli

# Valider les fichiers Markdown
markdownlint *.md
```

### 4. **validate-sql** 
V√©rifie l'existence des scripts SQL.

**√âtapes :**
- Liste les fichiers `.sql` dans `postgres/`

### 5. **integration-test** 
Test d'int√©gration complet du pipeline Docker.

**√âtapes :**
1. Cr√©ation du fichier `.env` depuis `.env.example`
2. Build des images Docker (`docker compose build`)
3. D√©marrage des services core (postgres, mongo, kafka)
4. Attente de 30 secondes pour le d√©marrage
5. V√©rification que les services sont en cours d'ex√©cution
6. Cleanup (arr√™t et suppression des volumes)

**Commande locale :**
```bash
docker compose build
docker compose up -d postgres mongo kafka
docker compose ps
docker compose down -v
```

### 6. **security-scan** 
Scan de s√©curit√© avec Trivy.

**√âtapes :**
- Scan des fichiers de configuration pour d√©tecter les vuln√©rabilit√©s critiques et √©lev√©es

**Commande locale :**
```bash
# Installer Trivy
# https://aquasecurity.github.io/trivy/latest/getting-started/installation/

trivy config .
```

## Am√©liorations par Rapport √† l'Ancien Workflow

| Aspect | Ancien | Nouveau |
|--------|--------|---------|
| **Docker Compose** | `docker-compose` (v1) - | `docker compose` (v2) - |
| **Validation Python** | DAGs uniquement | DAGs + Spark + src + producer - |
| **Tests d'int√©gration** | Aucun | Build + Run + Health check - |
| **S√©curit√©** | Aucun | Trivy vulnerability scan - |
| **Documentation** | Aucun | Validation README + RAPPORT - |
| **Jobs parall√®les** | Non | Oui (4 jobs en parall√®le) - |
| **Badge CI** | Non | Oui (dans README) - |

## R√©solution du Probl√®me Initial

**Erreur originale :**
```
docker-compose: command not found
Error: Process completed with exit code 127.
```

**Cause :** GitHub Actions runners (Ubuntu latest) n'ont plus `docker-compose` v1 install√© par d√©faut depuis 2023.

**Solution :** Utilisation de `docker compose` (v2) qui est int√©gr√© nativement √† Docker Engine.

## Utilisation

### D√©clenchement Automatique

Le workflow se d√©clenche automatiquement sur :
- Push vers `main` ou `master`
- Pull request vers `main` ou `master`

### D√©clenchement Manuel

Depuis l'interface GitHub :
1. Aller dans l'onglet **Actions**
2. S√©lectionner **Big Data Platform CI/CD**
3. Cliquer sur **Run workflow**

### Visualisation des R√©sultats

**Badge CI dans README :**
```markdown
[![Big Data Platform CI/CD](https://github.com/KerrianS/BigDataModelisation/actions/workflows/ci.yml/badge.svg)](https://github.com/KerrianS/BigDataModelisation/actions/workflows/ci.yml)
```

**Statuts possibles :**
- **Passing** : Tous les jobs ont r√©ussi
- **Failing** : Au moins un job a √©chou√©
- **Running** : Workflow en cours d'ex√©cution

## Debugging

### Voir les Logs d'un Job

1. Aller dans **Actions** > S√©lectionner le workflow
2. Cliquer sur le job qui a √©chou√©
3. D√©velopper les √©tapes pour voir les logs d√©taill√©s

### Reproduire Localement

```bash
# Valider la config Docker
docker compose config

# Valider le code Python
pip install apache-airflow==2.7.2 pyspark==3.5.0
python -m py_compile airflow/dags/*.py

# Tester l'int√©gration
docker compose up -d postgres mongo kafka
docker compose ps
docker compose down -v
```

## Prochaines √âtapes

**Am√©liorations futures :**
- [ ] Ajouter des tests unitaires Python (pytest)
- [ ] Impl√©menter des tests de performance (load testing)
- [ ] Ajouter un job de d√©ploiement automatique (CD)
- [ ] Configurer des notifications Slack/Discord en cas d'√©chec
- [ ] Ajouter un cache pour les d√©pendances Python (acc√©l√©rer le build)

## R√©f√©rences

- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [Docker Compose v2 Migration](https://docs.docker.com/compose/migrate/)
- [Trivy Security Scanner](https://aquasecurity.github.io/trivy/)
