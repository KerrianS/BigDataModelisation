# Rapport de Projet Big Data - Crypto Analytics

## 1. Probl√©matique M√©tier

Dans un contexte de volatilit√© extr√™me des march√©s crypto-monnaies, les traders et analystes ont besoin d'outils fiables pour :
*   **Collecter** des donn√©es de prix en temps r√©el
*   **Historiser** les donn√©es pour analyse de tendances
*   **Monitorer** la sant√© du pipeline d'ingestion

Ce projet propose une **plateforme d'ingestion et d'analyse de donn√©es crypto** bas√©e sur des technologies Big Data modernes.

## 2. Architecture Globale

L'architecture repose sur une stack simplifi√©e et moderne :

*   **Orchestration** : Apache Airflow
*   **Stockage** : PostgreSQL
*   **Monitoring** : Grafana + Loki + Promtail
*   **Infrastructure** : Docker & Docker Compose

```mermaid
graph TD
    API[CoinCap API] -->|HTTP GET| DAG[Airflow DAG]
    DAG -->|INSERT| PG[(PostgreSQL)]
    DAG -.->|Logs| PROM[Promtail]
    PROM -->|Push| LOKI[Loki]
    PG -->|Query| GRAF[Grafana]
    LOKI -->|Logs| GRAF
```

## 3. Pipeline de Donn√©es

Le flux de donn√©es est orchestr√© par Airflow :

1.  **D√©clenchement** : Le DAG `crypto_ingestion_pipeline` s'ex√©cute toutes les 10 minutes.
2.  **Extraction** : Requ√™te HTTP vers l'API CoinCap pour r√©cup√©rer les prix de Bitcoin, Ethereum, Solana, etc.
3.  **Transformation** : Parsing du JSON et extraction des champs pertinents (prix, market cap, volume).
4.  **Chargement** : Insertion dans PostgreSQL (table `crypto_prices`).
5.  **Monitoring** : Les logs Airflow sont collect√©s par Promtail et envoy√©s √† Loki pour visualisation dans Grafana.

## 4. Mod√®le de Donn√©es

Les donn√©es brutes JSON de l'API CoinCap :
```json
{
    "id": "bitcoin",
    "symbol": "BTC",
    "name": "Bitcoin",
    "priceUsd": "43256.78",
    "marketCapUsd": "847123456789.12",
    "volumeUsd24Hr": "12345678901.23",
    "changePercent24Hr": "2.45"
}
```

Sont transform√©es en table PostgreSQL `crypto_prices` :

| Colonne              | Type           | Description                    |
|----------------------|----------------|--------------------------------|
| id                   | SERIAL (PK)    | Identifiant technique          |
| asset_id             | VARCHAR(50)    | ID de l'asset (bitcoin, etc.)  |
| symbol               | VARCHAR(20)    | Symbole (BTC, ETH...)          |
| name                 | VARCHAR(50)    | Nom complet                    |
| price_usd            | NUMERIC(20,8)  | Prix en USD                    |
| market_cap_usd       | NUMERIC(20,2)  | Capitalisation boursi√®re       |
| volume_24h_usd       | NUMERIC(20,2)  | Volume 24h                     |
| change_percent_24h   | NUMERIC(10,4)  | Variation 24h (%)              |
| ingestion_timestamp  | TIMESTAMP      | Date d'ingestion               |

## 5. R√©sultats et Conclusions

Ce projet d√©montre la faisabilit√© d'une **architecture Big Data simplifi√©e** pour l'ingestion de donn√©es financi√®res :

‚úÖ **Avantages** :
*   Stack l√©g√®re (pas de Kafka/Spark pour ce cas d'usage)
*   Orchestration robuste avec Airflow
*   Monitoring int√©gr√© avec Grafana
*   Facilit√© de d√©ploiement (Docker Compose)

üîß **√âvolutions possibles** :
*   Ajout de Spark pour traitement batch complexe
*   Int√©gration d'un Data Lake (MongoDB) pour stockage brut
*   Alerting automatique sur variations de prix
*   Dashboard Grafana temps r√©el

L'approche "Infrastructure as Code" avec Docker permet un d√©ploiement rapide et reproductible en environnement de production.
